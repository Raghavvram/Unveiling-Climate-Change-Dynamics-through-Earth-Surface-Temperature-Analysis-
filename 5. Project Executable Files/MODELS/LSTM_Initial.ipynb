{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5a5499-930a-4bfb-806e-330937616574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout, Input\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99487b9-cf34-45cd-85ca-6fdd53eb6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "file_path = 'GlobalTemperatures.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Impute LandAverageTemperature and LandAverageTemperatureUncertainty with mean\n",
    "df['LandAverageTemperature'].fillna(df['LandAverageTemperature'].mean(), inplace=True)\n",
    "df['LandAverageTemperatureUncertainty'].fillna(df['LandAverageTemperatureUncertainty'].mean(), inplace=True)\n",
    "\n",
    "# For columns with 1200 missing values, drop those rows\n",
    "cols_to_dropna = ['LandMaxTemperature', 'LandMaxTemperatureUncertainty', 'LandMinTemperature', 'LandMinTemperatureUncertainty', 'LandAndOceanAverageTemperature', 'LandAndOceanAverageTemperatureUncertainty']\n",
    "df.dropna(subset=cols_to_dropna, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63bd3778-f278-47ac-b08b-6c85c12ba48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (1593, 9, 1)\n",
      "X_test_scaled shape: (399, 9, 1)\n",
      "y_train_scaled shape: (1593, 1)\n",
      "y_test_scaled shape: (399, 1)\n"
     ]
    }
   ],
   "source": [
    "# Add Year and Month columns based on 'dt' column\n",
    "df['Year'] = pd.to_datetime(df['dt']).dt.year\n",
    "df['Month'] = pd.to_datetime(df['dt']).dt.month\n",
    "\n",
    "# Prepare X (features) and y (target)\n",
    "X = df.drop(['LandAverageTemperature', 'dt'], axis=1)\n",
    "y = df['LandAverageTemperature']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale X and y using MinMaxScaler\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Only transform the testing data\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Reshape X_train_scaled and X_test_scaled for RNN input\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"y_train_scaled shape:\", y_train_scaled.shape)\n",
    "print(\"y_test_scaled shape:\", y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1518f7e2-4ad9-4e86-b4be-014f0bf1a462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,625</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │          \u001b[38;5;34m66,560\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m49,408\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)                  │           \u001b[38;5;34m1,625\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m26\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,619</span> (459.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m117,619\u001b[0m (459.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,619</span> (459.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m117,619\u001b[0m (459.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train_scaled.shape[1], 1)))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f061147a-db3e-4c3a-a27d-730b5ae78164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 - 3s - 72ms/step - loss: 0.0628 - val_loss: 0.0238\n",
      "Epoch 2/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0191 - val_loss: 0.0150\n",
      "Epoch 3/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0164 - val_loss: 0.0138\n",
      "Epoch 4/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0127 - val_loss: 0.0097\n",
      "Epoch 5/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 6/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0121 - val_loss: 0.0072\n",
      "Epoch 7/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 8/100\n",
      "40/40 - 0s - 10ms/step - loss: 0.0064 - val_loss: 0.0056\n",
      "Epoch 9/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 10/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0048 - val_loss: 0.0048\n",
      "Epoch 11/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 12/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 13/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0043 - val_loss: 0.0052\n",
      "Epoch 14/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 15/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 18/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0040 - val_loss: 0.0045\n",
      "Epoch 19/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 20/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 21/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0041 - val_loss: 0.0047\n",
      "Epoch 22/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0036 - val_loss: 0.0061\n",
      "Epoch 23/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 24/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 26/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0036 - val_loss: 0.0049\n",
      "Epoch 27/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 28/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 29/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 30/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0044 - val_loss: 0.0032\n",
      "Epoch 31/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 32/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0031 - val_loss: 0.0061\n",
      "Epoch 33/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0035 - val_loss: 0.0054\n",
      "Epoch 34/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 35/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 36/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0027 - val_loss: 0.0032\n",
      "Epoch 37/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0027 - val_loss: 0.0031\n",
      "Epoch 38/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0028 - val_loss: 0.0035\n",
      "Epoch 39/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 40/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 41/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 42/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 43/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 44/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 45/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 46/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 47/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 48/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0028 - val_loss: 0.0034\n",
      "Epoch 49/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0023 - val_loss: 0.0031\n",
      "Epoch 50/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 51/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 52/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0025 - val_loss: 0.0033\n",
      "Epoch 53/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 54/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0029 - val_loss: 0.0035\n",
      "Epoch 55/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 56/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 57/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 58/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0029 - val_loss: 0.0026\n",
      "Epoch 59/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 60/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0026 - val_loss: 0.0033\n",
      "Epoch 61/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 62/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 63/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 64/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 65/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 66/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 67/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 68/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 69/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 70/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 71/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 72/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0025 - val_loss: 0.0030\n",
      "Epoch 73/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 74/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 75/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 76/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 77/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 78/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 79/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 80/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 81/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0025 - val_loss: 0.0033\n",
      "Epoch 82/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 83/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 84/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 85/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 86/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 87/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 88/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 89/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0021 - val_loss: 0.0031\n",
      "Epoch 90/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 91/100\n",
      "40/40 - 0s - 9ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 92/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 93/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 94/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 95/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 96/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 97/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 98/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 99/100\n",
      "40/40 - 0s - 8ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 100/100\n",
      "40/40 - 0s - 7ms/step - loss: 0.0021 - val_loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32, validation_split=0.2,verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589baada-df35-4060-badc-638702b6952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "predictions = scaler_y.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f32349-d821-489b-ad08-1021d4ca8885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 3.88, Predicted: 3.928748369216919\n",
      "Actual: 8.689, Predicted: 8.596373558044434\n",
      "Actual: 13.622, Predicted: 13.261086463928223\n",
      "Actual: 2.335, Predicted: 2.5791003704071045\n",
      "Actual: 5.952999999999999, Predicted: 5.891555309295654\n",
      "Actual: 4.869, Predicted: 4.730517387390137\n",
      "Actual: 14.768, Predicted: 14.575563430786133\n",
      "Actual: 7.423999999999999, Predicted: 7.562475204467773\n",
      "Actual: 4.103, Predicted: 4.206771373748779\n",
      "Actual: 4.519, Predicted: 4.805171489715576\n",
      "Actual: 14.021, Predicted: 13.837247848510742\n",
      "Actual: 14.034, Predicted: 14.062273025512695\n",
      "Actual: 4.85, Predicted: 5.164228439331055\n",
      "Actual: 9.453, Predicted: 9.453171730041504\n",
      "Actual: 6.222, Predicted: 6.202760219573975\n",
      "Actual: 14.445, Predicted: 14.319841384887695\n",
      "Actual: 11.062, Predicted: 10.153813362121582\n",
      "Actual: 2.455, Predicted: 2.140364170074463\n",
      "Actual: 6.273, Predicted: 6.394965171813965\n",
      "Actual: 4.303, Predicted: 4.324885845184326\n",
      "Actual: 5.066, Predicted: 5.968013286590576\n",
      "Actual: 10.941999999999998, Predicted: 10.812159538269043\n",
      "Actual: 4.859, Predicted: 4.954249858856201\n",
      "Actual: 3.629, Predicted: 3.663015365600586\n",
      "Actual: 4.924, Predicted: 4.607018947601318\n",
      "Actual: 12.048, Predicted: 12.122894287109375\n",
      "Actual: 13.157, Predicted: 13.380535125732422\n",
      "Actual: 10.944, Predicted: 10.640525817871094\n",
      "Actual: 9.264, Predicted: 9.253933906555176\n",
      "Actual: 8.498999999999999, Predicted: 8.39823055267334\n",
      "Actual: 5.902, Predicted: 5.27003812789917\n",
      "Actual: 13.775, Predicted: 13.619460105895996\n",
      "Actual: 2.488, Predicted: 2.821749687194824\n",
      "Actual: 3.628, Predicted: 3.7773988246917725\n",
      "Actual: 14.201, Predicted: 14.162575721740723\n",
      "Actual: 13.22, Predicted: 13.259654998779297\n",
      "Actual: 5.186, Predicted: 6.87766170501709\n",
      "Actual: 9.338, Predicted: 9.292990684509277\n",
      "Actual: 3.422, Predicted: 3.6365668773651123\n",
      "Actual: 5.118, Predicted: 4.378202438354492\n",
      "Actual: 13.687, Predicted: 13.490196228027344\n",
      "Actual: 3.069, Predicted: 3.297368049621582\n",
      "Actual: 12.324000000000002, Predicted: 12.469572067260742\n",
      "Actual: 13.807, Predicted: 13.690370559692383\n",
      "Actual: 1.836, Predicted: 1.804073691368103\n",
      "Actual: 5.702000000000001, Predicted: 5.753643989562988\n",
      "Actual: 13.401, Predicted: 13.818764686584473\n",
      "Actual: 11.892, Predicted: 13.68375015258789\n",
      "Actual: 12.984000000000002, Predicted: 12.383249282836914\n",
      "Actual: 8.542, Predicted: 7.970729351043701\n",
      "Actual: 5.031000000000001, Predicted: 4.781020641326904\n",
      "Actual: 11.514, Predicted: 12.190479278564453\n",
      "Actual: 4.8660000000000005, Predicted: 4.95588493347168\n",
      "Actual: 12.095999999999998, Predicted: 12.251534461975098\n",
      "Actual: 14.285, Predicted: 14.285796165466309\n",
      "Actual: 3.2430000000000003, Predicted: 3.464186906814575\n",
      "Actual: 9.735, Predicted: 9.742073059082031\n",
      "Actual: 8.511000000000001, Predicted: 8.789870262145996\n",
      "Actual: 12.856, Predicted: 12.602660179138184\n",
      "Actual: 14.144, Predicted: 14.064966201782227\n",
      "Actual: 3.656, Predicted: 3.908674716949463\n",
      "Actual: 13.164, Predicted: 13.301539421081543\n",
      "Actual: 12.093, Predicted: 12.511902809143066\n",
      "Actual: 2.781, Predicted: 2.9243874549865723\n",
      "Actual: 11.831, Predicted: 12.19660758972168\n",
      "Actual: 13.405, Predicted: 13.197897911071777\n",
      "Actual: 13.752, Predicted: 13.415769577026367\n",
      "Actual: 14.052, Predicted: 13.839548110961914\n",
      "Actual: 9.488, Predicted: 9.481529235839844\n",
      "Actual: 4.8260000000000005, Predicted: 4.824700355529785\n",
      "Actual: 13.807, Predicted: 13.792373657226562\n",
      "Actual: 5.89, Predicted: 5.702927589416504\n",
      "Actual: 4.959, Predicted: 4.836441993713379\n",
      "Actual: 14.392, Predicted: 14.232657432556152\n",
      "Actual: 8.25, Predicted: 7.439899921417236\n",
      "Actual: 14.742, Predicted: 14.239870071411133\n",
      "Actual: 8.443999999999999, Predicted: 11.7992582321167\n",
      "Actual: 3.1950000000000003, Predicted: 3.4408416748046875\n",
      "Actual: 11.604, Predicted: 11.882803916931152\n",
      "Actual: 13.105, Predicted: 13.014687538146973\n",
      "Actual: 13.412, Predicted: 13.369096755981445\n",
      "Actual: 7.9209999999999985, Predicted: 5.602521896362305\n",
      "Actual: 2.052, Predicted: 2.693323850631714\n",
      "Actual: 3.5980000000000003, Predicted: 3.981328010559082\n",
      "Actual: 14.138, Predicted: 13.725383758544922\n",
      "Actual: 14.375, Predicted: 14.081314086914062\n",
      "Actual: 9.552, Predicted: 9.831196784973145\n",
      "Actual: 4.134, Predicted: 4.267826080322266\n",
      "Actual: 11.155, Predicted: 11.284826278686523\n",
      "Actual: 9.729, Predicted: 9.760101318359375\n",
      "Actual: 3.306, Predicted: 3.6920487880706787\n",
      "Actual: 14.140999999999998, Predicted: 13.809455871582031\n",
      "Actual: 14.377, Predicted: 14.252622604370117\n",
      "Actual: 13.796, Predicted: 13.876702308654785\n",
      "Actual: 9.671, Predicted: 9.93962574005127\n",
      "Actual: 7.329999999999999, Predicted: 6.650115489959717\n",
      "Actual: 6.404, Predicted: 6.4511590003967285\n",
      "Actual: 1.793, Predicted: 2.575411558151245\n",
      "Actual: 10.787999999999998, Predicted: 11.223982810974121\n",
      "Actual: 14.630999999999998, Predicted: 14.328161239624023\n",
      "Actual: 14.188, Predicted: 14.110918045043945\n",
      "Actual: 7.738999999999999, Predicted: 6.929688453674316\n",
      "Actual: 2.228, Predicted: 4.497256755828857\n",
      "Actual: 8.280999999999999, Predicted: 7.467733383178711\n",
      "Actual: 11.984000000000002, Predicted: 11.840835571289062\n",
      "Actual: 3.458, Predicted: 3.928651809692383\n",
      "Actual: 6.295, Predicted: 6.3134260177612305\n",
      "Actual: 2.291, Predicted: 2.539898157119751\n",
      "Actual: 9.006, Predicted: 7.6981730461120605\n",
      "Actual: 13.351, Predicted: 13.345190048217773\n",
      "Actual: 14.144, Predicted: 13.959665298461914\n",
      "Actual: 14.37, Predicted: 14.229079246520996\n",
      "Actual: 6.377999999999999, Predicted: 6.12949275970459\n",
      "Actual: 9.584, Predicted: 9.66943359375\n",
      "Actual: 14.255999999999998, Predicted: 14.217081069946289\n",
      "Actual: 1.966, Predicted: 2.466996669769287\n",
      "Actual: 13.514, Predicted: 13.455534934997559\n",
      "Actual: 4.259, Predicted: 4.629479885101318\n",
      "Actual: 2.713, Predicted: 3.049046754837036\n",
      "Actual: 12.153, Predicted: 12.399181365966797\n",
      "Actual: 9.281, Predicted: 9.436294555664062\n",
      "Actual: 11.763, Predicted: 12.454568862915039\n",
      "Actual: 4.016, Predicted: 4.458693981170654\n",
      "Actual: 14.505, Predicted: 14.389266014099121\n",
      "Actual: 3.604, Predicted: 3.584604263305664\n",
      "Actual: 3.921, Predicted: 3.9889585971832275\n",
      "Actual: 3.196, Predicted: 3.606210708618164\n",
      "Actual: 2.172, Predicted: 2.1715190410614014\n",
      "Actual: 9.516, Predicted: 9.356073379516602\n",
      "Actual: 14.231, Predicted: 14.034360885620117\n",
      "Actual: 11.685, Predicted: 11.450178146362305\n",
      "Actual: 11.354, Predicted: 11.529154777526855\n",
      "Actual: 3.071, Predicted: 3.0680127143859863\n",
      "Actual: 14.798, Predicted: 14.472658157348633\n",
      "Actual: 12.872, Predicted: 12.832819938659668\n",
      "Actual: 11.097, Predicted: 11.241399765014648\n",
      "Actual: 5.077, Predicted: 4.956384181976318\n",
      "Actual: 5.809, Predicted: 4.130921363830566\n",
      "Actual: 8.722000000000001, Predicted: 7.670162677764893\n",
      "Actual: 8.991999999999997, Predicted: 9.200214385986328\n",
      "Actual: 7.1560000000000015, Predicted: 7.27355432510376\n",
      "Actual: 3.832, Predicted: 4.115023612976074\n",
      "Actual: 3.155, Predicted: 2.557969570159912\n",
      "Actual: 3.3360000000000003, Predicted: 3.528102159500122\n",
      "Actual: 11.845, Predicted: 12.0209379196167\n",
      "Actual: 14.429, Predicted: 14.174898147583008\n",
      "Actual: 2.077, Predicted: 3.6382899284362793\n",
      "Actual: 3.5, Predicted: 3.923746109008789\n",
      "Actual: 11.729, Predicted: 11.89753246307373\n",
      "Actual: 3.092, Predicted: 2.74157452583313\n",
      "Actual: 6.3, Predicted: 6.134335041046143\n",
      "Actual: 13.395, Predicted: 13.019960403442383\n",
      "Actual: 7.486999999999999, Predicted: 5.958320617675781\n",
      "Actual: 3.24, Predicted: 3.6854662895202637\n",
      "Actual: 13.23, Predicted: 12.892427444458008\n",
      "Actual: 14.298, Predicted: 13.779719352722168\n",
      "Actual: 1.003, Predicted: 0.5203424692153931\n",
      "Actual: 2.7060000000000004, Predicted: 4.811618328094482\n",
      "Actual: 2.466, Predicted: 2.881467819213867\n",
      "Actual: 2.611, Predicted: 3.173374891281128\n",
      "Actual: 4.622, Predicted: 4.095290184020996\n",
      "Actual: 4.94, Predicted: 4.6744537353515625\n",
      "Actual: 13.537, Predicted: 13.04601001739502\n",
      "Actual: 4.811, Predicted: 4.356346607208252\n",
      "Actual: 2.286, Predicted: 2.231767416000366\n",
      "Actual: 3.8630000000000004, Predicted: 4.218865871429443\n",
      "Actual: 13.347, Predicted: 13.172390937805176\n",
      "Actual: 9.077, Predicted: 8.907506942749023\n",
      "Actual: 9.157, Predicted: 9.123133659362793\n",
      "Actual: 9.67, Predicted: 9.590612411499023\n",
      "Actual: 3.5, Predicted: 3.712059497833252\n",
      "Actual: 11.759, Predicted: 11.709386825561523\n",
      "Actual: 13.505999999999998, Predicted: 14.241963386535645\n",
      "Actual: 11.386, Predicted: 12.074819564819336\n",
      "Actual: 14.122, Predicted: 13.790491104125977\n",
      "Actual: 14.408, Predicted: 14.200133323669434\n",
      "Actual: 11.743, Predicted: 11.551006317138672\n",
      "Actual: 11.235, Predicted: 11.18139934539795\n",
      "Actual: 11.284999999999998, Predicted: 10.986942291259766\n",
      "Actual: 4.059, Predicted: 4.355624675750732\n",
      "Actual: 5.399, Predicted: 5.106575012207031\n",
      "Actual: 6.205, Predicted: 6.258731365203857\n",
      "Actual: 6.74, Predicted: 6.8204874992370605\n",
      "Actual: 7.748999999999999, Predicted: 7.376523494720459\n",
      "Actual: 3.513, Predicted: 3.604484796524048\n",
      "Actual: 10.69, Predicted: 9.675930976867676\n",
      "Actual: 13.331, Predicted: 13.090048789978027\n",
      "Actual: 5.097, Predicted: 5.117681980133057\n",
      "Actual: 13.713, Predicted: 13.671276092529297\n",
      "Actual: 1.941, Predicted: 2.4780325889587402\n",
      "Actual: 4.9190000000000005, Predicted: 4.952270030975342\n",
      "Actual: 13.557, Predicted: 13.47679328918457\n",
      "Actual: 2.71, Predicted: 3.0131328105926514\n",
      "Actual: 4.327999999999999, Predicted: 4.521710395812988\n",
      "Actual: 9.056, Predicted: 8.727330207824707\n",
      "Actual: 2.857, Predicted: 3.080996036529541\n",
      "Actual: 5.272, Predicted: 4.9455885887146\n",
      "Actual: 4.6850000000000005, Predicted: 4.98011589050293\n",
      "Actual: 6.397, Predicted: 6.236182689666748\n",
      "Actual: 7.185999999999999, Predicted: 6.973549842834473\n",
      "Actual: 13.484000000000002, Predicted: 14.169805526733398\n",
      "Actual: 14.223, Predicted: 14.55801010131836\n",
      "Actual: 10.611, Predicted: 10.281289100646973\n",
      "Actual: 9.627, Predicted: 9.642280578613281\n",
      "Actual: 3.14, Predicted: 4.663222312927246\n",
      "Actual: 14.238, Predicted: 13.563122749328613\n",
      "Actual: 8.58, Predicted: 4.389348030090332\n",
      "Actual: 11.222, Predicted: 11.652488708496094\n",
      "Actual: 11.797, Predicted: 13.052199363708496\n",
      "Actual: 13.964, Predicted: 13.879121780395508\n",
      "Actual: 3.4560000000000004, Predicted: 3.701415538787842\n",
      "Actual: 14.468, Predicted: 14.395941734313965\n",
      "Actual: 7.767999999999998, Predicted: 7.013210773468018\n",
      "Actual: 8.838, Predicted: 8.993906021118164\n",
      "Actual: 6.047999999999999, Predicted: 6.150308132171631\n",
      "Actual: 11.925, Predicted: 11.78080940246582\n",
      "Actual: 10.898, Predicted: 10.192928314208984\n",
      "Actual: 14.242, Predicted: 13.919934272766113\n",
      "Actual: 13.161, Predicted: 13.184843063354492\n",
      "Actual: 3.104, Predicted: 3.249328374862671\n",
      "Actual: 5.596, Predicted: 5.365283489227295\n",
      "Actual: 13.308, Predicted: 13.6099214553833\n",
      "Actual: 10.851, Predicted: 11.67983627319336\n",
      "Actual: 8.594999999999999, Predicted: 8.836058616638184\n",
      "Actual: 5.245, Predicted: 5.3263349533081055\n",
      "Actual: 7.487, Predicted: 7.5668439865112305\n",
      "Actual: 11.308, Predicted: 11.484719276428223\n",
      "Actual: 5.915, Predicted: 5.921946048736572\n",
      "Actual: 13.204, Predicted: 13.061052322387695\n",
      "Actual: 6.377999999999999, Predicted: 6.421581745147705\n",
      "Actual: 9.55, Predicted: 9.667574882507324\n",
      "Actual: 6.356, Predicted: 6.127875328063965\n",
      "Actual: 3.333, Predicted: 3.3524599075317383\n",
      "Actual: 8.870999999999997, Predicted: 9.062397003173828\n",
      "Actual: 2.616, Predicted: 2.902735471725464\n",
      "Actual: 3.3960000000000004, Predicted: 3.4523348808288574\n",
      "Actual: 2.989, Predicted: 2.784914493560791\n",
      "Actual: 3.33, Predicted: 3.5082337856292725\n",
      "Actual: 12.127999999999998, Predicted: 12.164987564086914\n",
      "Actual: 9.196, Predicted: 8.939031600952148\n",
      "Actual: 13.217, Predicted: 14.289582252502441\n",
      "Actual: 14.339, Predicted: 13.941731452941895\n",
      "Actual: 14.274, Predicted: 14.068188667297363\n",
      "Actual: 4.6, Predicted: 4.838242053985596\n",
      "Actual: 7.6839999999999975, Predicted: 7.405253887176514\n",
      "Actual: 9.021, Predicted: 8.931315422058105\n",
      "Actual: 13.531, Predicted: 13.422003746032715\n",
      "Actual: 13.577, Predicted: 13.540568351745605\n",
      "Actual: 3.005, Predicted: 3.486984968185425\n",
      "Actual: 15.212999999999997, Predicted: 14.821773529052734\n",
      "Actual: 12.146, Predicted: 12.306061744689941\n",
      "Actual: 7.617999999999999, Predicted: 7.176614284515381\n",
      "Actual: 14.827, Predicted: 14.532459259033203\n",
      "Actual: 11.989999999999998, Predicted: 12.07336139678955\n",
      "Actual: 13.365, Predicted: 13.54987907409668\n",
      "Actual: 10.803, Predicted: 10.452746391296387\n",
      "Actual: 11.239, Predicted: 10.842978477478027\n",
      "Actual: 3.559, Predicted: 3.840419292449951\n",
      "Actual: 14.019, Predicted: 13.628267288208008\n",
      "Actual: 13.824000000000002, Predicted: 13.627102851867676\n",
      "Actual: 11.478, Predicted: 12.379040718078613\n",
      "Actual: 2.0540000000000003, Predicted: 2.01714825630188\n",
      "Actual: 7.906999999999999, Predicted: 6.634341716766357\n",
      "Actual: 5.01, Predicted: 3.442488670349121\n",
      "Actual: 8.442, Predicted: 8.011527061462402\n",
      "Actual: 13.294, Predicted: 14.457711219787598\n",
      "Actual: 4.152, Predicted: 3.674144744873047\n",
      "Actual: 8.69, Predicted: 8.85281753540039\n",
      "Actual: 2.627, Predicted: 2.3524255752563477\n",
      "Actual: 9.378, Predicted: 9.356205940246582\n",
      "Actual: 13.77, Predicted: 13.962132453918457\n",
      "Actual: 4.222, Predicted: 4.270270347595215\n",
      "Actual: 14.226, Predicted: 13.913793563842773\n",
      "Actual: 9.12, Predicted: 8.878414154052734\n",
      "Actual: 3.069, Predicted: 3.1155998706817627\n",
      "Actual: 4.257, Predicted: 4.673771858215332\n",
      "Actual: 4.538, Predicted: 4.373936176300049\n",
      "Actual: 6.738, Predicted: 6.780354976654053\n",
      "Actual: 3.743, Predicted: 3.925607442855835\n",
      "Actual: 14.06, Predicted: 14.05085277557373\n",
      "Actual: 15.160999999999998, Predicted: 14.787602424621582\n",
      "Actual: 11.977, Predicted: 11.751742362976074\n",
      "Actual: 3.457, Predicted: 4.708512306213379\n",
      "Actual: 4.31, Predicted: 4.614291191101074\n",
      "Actual: 13.113, Predicted: 12.872655868530273\n",
      "Actual: 2.226, Predicted: 5.236990928649902\n",
      "Actual: 13.765, Predicted: 14.275808334350586\n",
      "Actual: 2.376, Predicted: 2.6493003368377686\n",
      "Actual: 7.994, Predicted: 7.561919212341309\n",
      "Actual: 3.039, Predicted: 2.9889955520629883\n",
      "Actual: 14.145, Predicted: 14.017233848571777\n",
      "Actual: 11.771999999999998, Predicted: 11.684697151184082\n",
      "Actual: 14.351, Predicted: 14.120816230773926\n",
      "Actual: 8.535, Predicted: 12.232612609863281\n",
      "Actual: 5.731, Predicted: 5.778861045837402\n",
      "Actual: 12.039, Predicted: 12.362458229064941\n",
      "Actual: 2.506, Predicted: 2.6124703884124756\n",
      "Actual: 8.247, Predicted: 8.042861938476562\n",
      "Actual: 10.53, Predicted: 10.871710777282715\n",
      "Actual: 3.687, Predicted: 3.8793985843658447\n",
      "Actual: 3.766, Predicted: 3.8283445835113525\n",
      "Actual: 3.7780000000000005, Predicted: 4.249870300292969\n",
      "Actual: 13.932, Predicted: 13.841391563415527\n",
      "Actual: 8.366, Predicted: 8.00820255279541\n",
      "Actual: 3.8220000000000005, Predicted: 4.289980888366699\n",
      "Actual: 5.973, Predicted: 5.875247001647949\n",
      "Actual: 14.512, Predicted: 14.58182430267334\n",
      "Actual: 5.247000000000001, Predicted: 5.173186302185059\n",
      "Actual: 11.338, Predicted: 11.159461975097656\n",
      "Actual: 3.74, Predicted: 4.043394565582275\n",
      "Actual: 5.827999999999999, Predicted: 5.7908220291137695\n",
      "Actual: 3.39, Predicted: 3.6281380653381348\n",
      "Actual: 2.862, Predicted: 3.1858997344970703\n",
      "Actual: 13.297, Predicted: 13.312312126159668\n",
      "Actual: 2.9560000000000004, Predicted: 3.2086477279663086\n",
      "Actual: 8.892000000000001, Predicted: 8.668744087219238\n",
      "Actual: 14.742, Predicted: 14.485292434692383\n",
      "Actual: 11.71, Predicted: 12.47285270690918\n",
      "Actual: 11.861999999999998, Predicted: 11.959665298461914\n",
      "Actual: 1.551, Predicted: 2.519106864929199\n",
      "Actual: 7.635999999999999, Predicted: 8.282735824584961\n",
      "Actual: 13.698, Predicted: 13.441113471984863\n",
      "Actual: 13.678, Predicted: 13.704378128051758\n",
      "Actual: 13.153, Predicted: 13.312447547912598\n",
      "Actual: 13.691, Predicted: 13.48982048034668\n",
      "Actual: 2.402, Predicted: 2.1949052810668945\n",
      "Actual: 12.199000000000002, Predicted: 12.28124713897705\n",
      "Actual: 4.506, Predicted: 4.63340950012207\n",
      "Actual: 13.298, Predicted: 13.158683776855469\n",
      "Actual: 14.231, Predicted: 13.83201789855957\n",
      "Actual: 8.738, Predicted: 8.810469627380371\n",
      "Actual: 8.679, Predicted: 8.662518501281738\n",
      "Actual: 4.038, Predicted: 4.362826824188232\n",
      "Actual: 14.09, Predicted: 13.913825035095215\n",
      "Actual: 13.214, Predicted: 13.623976707458496\n",
      "Actual: 3.898, Predicted: 3.8277604579925537\n",
      "Actual: 13.488, Predicted: 13.05356502532959\n",
      "Actual: 2.008, Predicted: 2.397029399871826\n",
      "Actual: 9.367, Predicted: 9.524802207946777\n",
      "Actual: 5.671, Predicted: 5.43579626083374\n",
      "Actual: 8.865, Predicted: 8.651981353759766\n",
      "Actual: 13.442, Predicted: 13.510027885437012\n",
      "Actual: 9.232, Predicted: 9.473346710205078\n",
      "Actual: 11.954, Predicted: 12.127708435058594\n",
      "Actual: 12.406, Predicted: 12.561128616333008\n",
      "Actual: 8.173, Predicted: 8.216286659240723\n",
      "Actual: 11.946, Predicted: 12.053770065307617\n",
      "Actual: 13.434, Predicted: 13.370914459228516\n",
      "Actual: 4.9990000000000006, Predicted: 3.9900052547454834\n",
      "Actual: 6.814, Predicted: 6.887560844421387\n",
      "Actual: 5.4510000000000005, Predicted: 5.530637741088867\n",
      "Actual: 7.76, Predicted: 9.343119621276855\n",
      "Actual: 5.318, Predicted: 5.45352029800415\n",
      "Actual: 8.445, Predicted: 8.378927230834961\n",
      "Actual: 14.139, Predicted: 14.002202033996582\n",
      "Actual: 2.609, Predicted: 2.903120517730713\n",
      "Actual: 6.183, Predicted: 5.844602584838867\n",
      "Actual: 14.31, Predicted: 13.872421264648438\n",
      "Actual: 14.151, Predicted: 14.577069282531738\n",
      "Actual: 3.491, Predicted: 3.7956204414367676\n",
      "Actual: 4.421, Predicted: 4.007967472076416\n",
      "Actual: 13.836, Predicted: 13.758320808410645\n",
      "Actual: 9.238, Predicted: 8.858647346496582\n",
      "Actual: 5.952000000000001, Predicted: 5.904990196228027\n",
      "Actual: 5.093, Predicted: 5.020968437194824\n",
      "Actual: 13.223, Predicted: 13.214069366455078\n",
      "Actual: 4.922, Predicted: 3.48891544342041\n",
      "Actual: 7.561999999999998, Predicted: 6.97166633605957\n",
      "Actual: 11.62, Predicted: 11.67630672454834\n",
      "Actual: 13.547, Predicted: 13.486482620239258\n",
      "Actual: 12.03, Predicted: 11.938672065734863\n",
      "Actual: 14.004, Predicted: 13.711854934692383\n",
      "Actual: 4.704, Predicted: 4.7787885665893555\n",
      "Actual: 4.026, Predicted: 4.239687442779541\n",
      "Actual: 8.588999999999999, Predicted: 8.099647521972656\n",
      "Actual: 2.905, Predicted: 3.178290367126465\n",
      "Actual: 14.349, Predicted: 14.138138771057129\n",
      "Actual: 8.434, Predicted: 8.204950332641602\n",
      "Actual: 13.296, Predicted: 12.939680099487305\n",
      "Actual: 3.265, Predicted: 3.5251173973083496\n",
      "Actual: 14.059, Predicted: 13.921097755432129\n",
      "Actual: 11.665, Predicted: 11.375295639038086\n",
      "Actual: 11.867, Predicted: 12.280340194702148\n",
      "Actual: 13.475, Predicted: 13.571554183959961\n",
      "Actual: 8.097999999999999, Predicted: 7.600502014160156\n",
      "Actual: 13.789, Predicted: 13.608800888061523\n",
      "Actual: 2.492, Predicted: 3.089505434036255\n",
      "Actual: 3.7850000000000006, Predicted: 4.165407180786133\n",
      "Actual: 13.896, Predicted: 13.929248809814453\n",
      "Actual: 5.405, Predicted: 5.2043938636779785\n",
      "Actual: 13.478, Predicted: 13.46346664428711\n",
      "Actual: 2.9410000000000003, Predicted: 3.2956855297088623\n",
      "Actual: 2.6790000000000003, Predicted: 3.054486036300659\n",
      "Actual: 5.524, Predicted: 5.699197292327881\n",
      "Actual: 13.769, Predicted: 13.599743843078613\n",
      "Actual: 9.096, Predicted: 8.5615873336792\n",
      "Actual: 2.71, Predicted: 3.0107293128967285\n",
      "Actual: 12.03, Predicted: 12.142838478088379\n",
      "Actual: 2.082, Predicted: 2.2994563579559326\n"
     ]
    }
   ],
   "source": [
    "# Compare predictions with actual values\n",
    "actual = scaler_y.inverse_transform(y_test_scaled)\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"Actual: {actual[i][0]}, Predicted: {predictions[i][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13167292-bb8c-4f58-83c6-985362224888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.3429270513864388\n",
      "Mean Squared Error (MSE): 0.3456430696371938\n",
      "Root Mean Squared Error (RMSE): 0.5879141685970783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(actual, predictions)\n",
    "mse = mean_squared_error(actual, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
